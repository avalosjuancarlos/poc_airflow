# ğŸš€ Airflow Market Data Pipeline

<div align="center">

[![Airflow](https://img.shields.io/badge/Airflow-2.11.0-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.10-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/Tests-82%20Passing-success?style=for-the-badge&logo=pytest)](#-testing)
[![Coverage](https://img.shields.io/badge/Coverage-84%25-success?style=for-the-badge&logo=codecov)](#-testing)

**Production-ready Apache Airflow setup with CeleryExecutor, comprehensive testing, and enterprise-grade logging.**

[Features](#-key-features) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Architecture](#-architecture) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Quick Start](#-quick-start)
- [Documentation](#-documentation)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Configuration](#%EF%B8%8F-configuration)
- [Testing](#-testing)
- [Monitoring & Logging](#-monitoring--logging)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Overview

Enterprise-ready Apache Airflow 2.11 deployment with:

- **ğŸ”„ CeleryExecutor** for distributed task execution
- **ğŸ“Š Market Data DAG** for fetching financial data from Yahoo Finance API
- **ğŸ§ª Comprehensive Testing** with 82 tests and 84% coverage
- **ğŸ“ Centralized Logging** with Sentry/Datadog integration ready
- **ğŸ³ Docker Compose** setup for local development and testing
- **âœ… CI/CD Pipeline** with automated testing and linting
- **ğŸ“š Complete Documentation** for users, developers, and operators

---

## âœ¨ Key Features

### ğŸ—ï¸ Infrastructure

| Feature | Description |
|---------|-------------|
| **PostgreSQL 13** | Metadata database with persistent storage |
| **Redis 7.2** | Message broker for Celery task queue |
| **CeleryExecutor** | Horizontally scalable task execution |
| **Flower** | Web-based Celery monitoring (optional) |
| **Docker Compose** | One-command deployment |

### ğŸ’¼ Market Data Pipeline (ETL)

#### Extract
- âœ… **Yahoo Finance Integration** - Real-time market data API
- âœ… **Smart Timestamp Logic** - Handles current day vs historical data
- âœ… **Rate Limiting Handling** - Automatic retry with exponential backoff
- âœ… **API Health Sensor** - Proactive availability checking
- âœ… **Multi-Date Fetch** - Fetch 1-20 dates with resilient error handling

#### Transform
- âœ… **12 Technical Indicators** - SMA, RSI, MACD, Bollinger Bands, Volatility, Returns
- âœ… **Data Validation** - Numeric conversion and validation
- âœ… **Pandas Processing** - Efficient DataFrame operations
- âœ… **NaN Handling** - Graceful handling of weekends and missing data

#### Load
- âœ… **Parquet Storage** - Apache Parquet with Snappy compression
- âœ… **Append Mode** - Automatic deduplication by date
- âœ… **Persistent Storage** - Docker volume for data retention
- âœ… **Automatic Backfill** - 20-day backfill on first execution

#### Additional
- âœ… **Daily Automation** - `@daily` schedule (00:00 UTC)
- âœ… **Configurable Parameters** - Environment variables and Airflow Variables
- âœ… **Comprehensive Error Handling** - Multi-level retry logic and logging

### ğŸ”§ Developer Experience

- âœ… **Modular Architecture** - Organized into config, utils, operators, sensors, transformers, storage
- âœ… **131 Unit + Integration Tests** - High test coverage (89%)
- âœ… **Type Hints** - Full Python type annotations
- âœ… **Linting & Formatting** - Black, isort, flake8 enforcement
- âœ… **CI/CD Pipeline** - GitHub Actions automated testing
- âœ… **Local Testing** - Docker Compose test environment

### ğŸ“Š Logging & Monitoring

- âœ… **Centralized Logger** - Custom `MarketDataLogger` class
- âœ… **Structured Logging** - Contextual information in every log
- âœ… **Execution Decorators** - Automatic timing and error logging
- âœ… **Metrics Tracking** - Built-in metrics for monitoring
- âœ… **Audit Logging** - Compliance-ready audit trail
- âœ… **Sentry Integration** - Error tracking (optional)
- âœ… **Datadog Integration** - APM and metrics (optional)

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker Desktop** or Docker Engine (v20.10+)
- **Docker Compose** (v2.0+)
- **4GB RAM** minimum (8GB recommended)
- **2 CPU cores** minimum (4+ recommended)

### 1. Clone and Setup

```bash
# Clone the repository
git clone <repository-url>
cd poc_airflow

# Copy environment template
cp env.template .env

# (Linux only) Set Airflow UID
echo "AIRFLOW_UID=$(id -u)" >> .env
```

### 2. Initialize Airflow

```bash
# Initialize database and create admin user
docker compose up airflow-init
```

### 3. Start Services

```bash
# Start all services
docker compose up -d

# Optional: Start with Flower monitoring
docker compose --profile flower up -d
```

### 4. Access Airflow

Open your browser at **http://localhost:8080**

- **Username**: `airflow`
- **Password**: `airflow`

**Optional - Flower (Celery Monitor)**: http://localhost:5555

### 5. Run Your First DAG

The **`get_market_data`** DAG is ready to use:

1. Go to the DAGs page
2. Toggle the `get_market_data` DAG to **ON**
3. Click **â–¶ï¸ Trigger DAG** 
4. Optionally, configure parameters (ticker, date)
5. Monitor execution in the Graph or Gantt view

---

## ğŸ“š Documentation

Comprehensive documentation organized by audience:

### ğŸš¦ Getting Started
- **[Installation Guide](docs/getting-started/installation.md)** - Detailed setup instructions
- **[Quick Start Tutorial](docs/getting-started/quick-start.md)** - Your first DAG in 5 minutes
- See [Configuration Guide](docs/user-guide/configuration.md) for configuration options

### ğŸ‘¤ User Guide
- **[Market Data DAG](docs/user-guide/market-data-dag.md)** - Using the Yahoo Finance DAG
- **[Data Warehouse](docs/user-guide/data-warehouse.md)** - Multi-environment warehouse guide
- **[Configuration Options](docs/user-guide/configuration.md)** - All configurable parameters
- **[Airflow Variables](docs/user-guide/airflow-variables.md)** - Dynamic configuration
- **[Logging Guide](docs/user-guide/logging.md)** - Understanding logs

### ğŸ‘¨â€ğŸ’» Developer Guide
- **[Architecture Overview](docs/architecture/overview.md)** - Complete system design
- **[Testing Guide](docs/developer-guide/testing.md)** - Running and writing tests
- **[API Reference](docs/developer-guide/api-reference.md)** - Complete module documentation
- **[Code Style](docs/developer-guide/code-style.md)** - Standards and conventions
- **[Contributing](docs/developer-guide/contributing.md)** - How to contribute

### âš™ï¸ Operations Guide
- **[Deployment](docs/operations/deployment.md)** - Production deployment guide
- **[Monitoring](docs/operations/monitoring.md)** - Observability and alerting
- **[Troubleshooting](docs/operations/troubleshooting.md)** - Common issues and solutions
- **[Migration](docs/operations/migration-guide.md)** - Environment and version migration
- **[Performance Tuning](docs/operations/performance-tuning.md)** - Optimization guide
- **[Security](docs/SECURITY.md)** - Security best practices

### ğŸ“– Reference
- **[Environment Variables](docs/reference/environment-variables.md)** - Complete env var reference
- **[CLI Commands](docs/reference/cli-commands.md)** - Comprehensive CLI reference  
- **[FAQs](docs/reference/faq.md)** - Frequently asked questions
- See [Useful Commands](#-useful-commands) section below for quick reference

---

## ğŸ—ï¸ Architecture

### System Components

```mermaid
graph TB
    UI[Web UI :8080] --> WS[Webserver]
    WS --> DB[(PostgreSQL<br/>:5432)]
    SCH[Scheduler] --> DB
    SCH --> REDIS[(Redis<br/>:6379)]
    REDIS --> W1[Worker 1]
    REDIS --> W2[Worker 2]
    REDIS --> W3[Worker N...]
    W1 --> DB
    W2 --> DB
    W3 --> DB
    TRG[Triggerer] --> DB
    FL[Flower :5555] --> REDIS
    
    style UI fill:#4A90E2
    style DB fill:#2ECC71
    style REDIS fill:#E74C3C
    style SCH fill:#F39C12
```

### Market Data DAG Flow (ETL Pipeline)

```mermaid
graph LR
    A[Start] --> B[Validate<br/>Ticker]
    B --> C[Determine<br/>Dates]
    C -->|No Parquet| D1[Backfill<br/>20 Days]
    C -->|Exists| D2[Single<br/>Day]
    D1 --> E[Check API<br/>Sensor]
    D2 --> E
    E -->|Available| F[Fetch Multiple<br/>Dates]
    E -->|Unavailable| G[Retry 30s<br/>Exp. Backoff]
    G --> E
    F --> H[Transform<br/>12 Indicators]
    H --> I[Save to<br/>Parquet]
    I --> J[End]
    
    F -->|Rate Limit| K[Retry with<br/>Backoff]
    K --> F
    
    style A fill:#2ECC71
    style J fill:#2ECC71
    style C fill:#9B59B6
    style E fill:#3498DB
    style F fill:#3498DB
    style H fill:#F39C12
    style I fill:#E74C3C
    style G fill:#E74C3C
    style K fill:#E74C3C
```

### Execution Flow

1. **Scheduler** reads DAG files and creates task instances
2. **Tasks** are queued in **Redis** (Celery queue)
3. **Workers** pick up tasks from the queue
4. **Workers** execute tasks and update status in **PostgreSQL**
5. **Webserver** displays real-time status from database
6. **Triggerer** handles asynchronous/deferrable tasks
7. **Flower** monitors Celery workers (optional)

---

## ğŸ“ Project Structure

```
poc_airflow/
â”œâ”€â”€ dags/                          # Airflow DAGs
â”‚   â”œâ”€â”€ get_market_data_dag.py    # Main ETL pipeline (@daily)
â”‚   â””â”€â”€ market_data/              # Modular DAG components
â”‚       â”œâ”€â”€ config/               # Configuration
â”‚       â”‚   â”œâ”€â”€ settings.py       # Settings management (triple fallback)
â”‚       â”‚   â””â”€â”€ logging_config.py # Logging configuration
â”‚       â”œâ”€â”€ operators/            # Custom operators
â”‚       â”‚   â”œâ”€â”€ market_data_operators.py  # Original operators
â”‚       â”‚   â””â”€â”€ transform_operators.py    # Transform & backfill logic
â”‚       â”œâ”€â”€ sensors/              # Custom sensors
â”‚       â”‚   â””â”€â”€ api_sensor.py     # API availability check
â”‚       â”œâ”€â”€ transformers/         # Data transformation ğŸ†•
â”‚       â”‚   â””â”€â”€ technical_indicators.py   # 12 technical indicators
â”‚       â”œâ”€â”€ storage/              # Data persistence ğŸ†•
â”‚       â”‚   â””â”€â”€ parquet_storage.py        # Parquet save/load
â”‚       â””â”€â”€ utils/                # Utilities
â”‚           â”œâ”€â”€ api_client.py     # Yahoo Finance client
â”‚           â”œâ”€â”€ validators.py     # Input validation
â”‚           â””â”€â”€ logger.py         # Centralized logging
â”‚
â”œâ”€â”€ data/                         # Parquet storage (persistent) ğŸ†•
â”‚   â””â”€â”€ {TICKER}_market_data.parquet
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ getting-started/          # Getting started guides
â”‚   â”œâ”€â”€ user-guide/              # User documentation
â”‚   â”œâ”€â”€ developer-guide/         # Developer documentation
â”‚   â”œâ”€â”€ archive/                 # Archived documentation
â”‚   â””â”€â”€ README.md                # Documentation index
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/                    # Unit tests (119 tests)
â”‚   â”œâ”€â”€ integration/             # Integration tests (12 tests)
â”‚   â””â”€â”€ conftest.py              # Pytest fixtures
â”‚
â”œâ”€â”€ logs/                         # Airflow logs (auto-generated)
â”œâ”€â”€ plugins/                      # Custom Airflow plugins
â”œâ”€â”€ config/                       # Additional config files
â”‚
â”œâ”€â”€ docker-compose.yml            # Main services configuration
â”œâ”€â”€ docker-compose.test.yml       # Testing environment
â”œâ”€â”€ env.template                  # Environment variables template
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ pytest.ini                    # Pytest configuration
â”œâ”€â”€ .flake8                       # Flake8 configuration
â”œâ”€â”€ .isort.cfg                    # Import sorting configuration
â””â”€â”€ README.md                     # This file
```

---

## âš™ï¸ Configuration

### Environment Variables

Edit `.env` file to customize your installation:

```bash
# Airflow Configuration
AIRFLOW_IMAGE_NAME=apache/airflow:2.11.0-python3.10
AIRFLOW_UID=50000
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true

# Admin Credentials
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# Market Data Configuration
MARKET_DATA_DEFAULT_TICKER=AAPL
YAHOO_FINANCE_API_BASE_URL=https://query2.finance.yahoo.com/v8/finance/chart
MARKET_DATA_API_TIMEOUT=30
MARKET_DATA_MAX_RETRIES=3

# Logging Configuration
ENVIRONMENT=development  # development|staging|production
AIRFLOW__LOGGING__LEVEL=INFO
AIRFLOW__LOGGING__JSON_FORMAT=false

# Optional: Monitoring Integration
# SENTRY_DSN=https://your-key@sentry.io/project
# DD_API_KEY=your-datadog-api-key
```

### Airflow Variables

Set dynamic configuration via Airflow UI or CLI:

```bash
# Via CLI
docker compose exec airflow-scheduler airflow variables set market_data_default_ticker TSLA

# Via script
./scripts/setup_airflow_variables.sh
```

**See**: [Configuration Guide](docs/user-guide/configuration.md) and [Airflow Variables Guide](docs/user-guide/airflow-variables.md) for complete details.

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Using Docker Compose
docker compose -f docker-compose.test.yml up test

# Unit tests only
docker compose -f docker-compose.test.yml up test-unit-only

# Integration tests only
docker compose -f docker-compose.test.yml up test-integration-only

# With coverage report
docker compose -f docker-compose.test.yml up test-coverage
```

### Run Linting

```bash
# Run all linters
docker compose -f docker-compose.test.yml up lint

# Individual linters
flake8 dags/market_data
black --check dags/market_data tests/
isort --check-only dags/market_data tests/
```

### Test Coverage

Current coverage: **84.22%**

- **Unit Tests**: 50 tests covering all modules
- **Integration Tests**: 14 tests for DAG workflows
- **Total**: 82 tests passing

**See**: [Testing Guide](docs/developer-guide/testing.md) for details.

---

## ğŸ“Š Monitoring & Logging

### Centralized Logging

The project includes a production-ready logging system:

```python
from market_data.utils import get_logger

logger = get_logger(__name__)

# Structured logging
logger.info("Processing data", extra={"ticker": "AAPL", "records": 100})

# Metrics tracking
logger.metric("api.response_time", 1.234, {"endpoint": "/chart"})

# Audit logging
logger.audit("data_fetched", {"user": "airflow", "ticker": "AAPL"})
```

**Features**:
- Contextual information in every log
- Automatic execution timing
- Metrics and audit trails
- Sentry/Datadog integration ready

**See**: [Logging Guide](docs/user-guide/logging.md)

### Monitoring with Flower

Monitor Celery workers in real-time:

```bash
# Start Flower
docker compose --profile flower up -d

# Access at http://localhost:5555
```

Monitor Celery workers and track performance metrics

---

## ğŸ”§ Useful Commands

### Service Management

```bash
# Start services
docker compose up -d

# Stop services
docker compose down

# Restart a service
docker compose restart airflow-worker

# Scale workers
docker compose up -d --scale airflow-worker=3

# View logs
docker compose logs -f airflow-scheduler
```

### Airflow CLI

```bash
# List DAGs
docker compose exec airflow-scheduler airflow dags list

# Test a DAG
docker compose exec airflow-scheduler airflow dags test get_market_data 2025-11-12

# List variables
docker compose exec airflow-scheduler airflow variables list
```

### Database Operations

```bash
# Backup database
docker compose exec postgres pg_dump -U airflow airflow > backup.sql

# Restore database
docker compose exec -T postgres psql -U airflow airflow < backup.sql
```

**See**: Full command reference in sections above

---

## ğŸ› Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Services not starting | Check `docker compose logs` |
| DAGs not appearing | Verify file in `dags/` and check scheduler logs |
| Port 8080 in use | Change port in `docker-compose.yml` |
| Permission errors (Linux) | Run `sudo chown -R $(id -u):$(id -g) dags logs` |
| Workers not picking tasks | Check Redis connection and restart workers |

For more help, see common solutions above or check Docker logs.

---

## ğŸ¤ Contributing

We welcome contributions! Follow the steps below to contribute.

### Quick Contribution Steps

1. **Fork** the repository
2. **Create a branch** (`git checkout -b feature/amazing-feature`)
3. **Make your changes** and add tests
4. **Run tests** (`docker compose -f docker-compose.test.yml up test`)
5. **Commit** (`git commit -m 'Add amazing feature'`)
6. **Push** (`git push origin feature/amazing-feature`)
7. **Open a Pull Request**

### Development Standards

- âœ… All tests must pass
- âœ… Code coverage > 70%
- âœ… Follow Black formatting
- âœ… Pass flake8 linting
- âœ… Add docstrings to new functions
- âœ… Update documentation as needed

---

## ğŸ“„ License

This project uses Apache Airflow, which is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

---

## ğŸŒŸ Acknowledgments

- **Apache Airflow** - Workflow orchestration platform
- **Yahoo Finance API** - Market data source
- **Docker** - Containerization platform

---

## ğŸ“ Support

- ğŸ“– [Documentation](docs/README.md)
- ğŸ› [Issue Tracker](https://github.com/avalosjuancarlos/poc_airflow/issues)
- ğŸ’¬ [Discussions](https://github.com/avalosjuancarlos/poc_airflow/discussions)
- ğŸ“§ Email: support@example.com

---

<div align="center">

**Built with â¤ï¸ using Apache Airflow**

[â¬† back to top](#-airflow-market-data-pipeline)

</div>
