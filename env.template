# ============================================================================
# Airflow Core Configuration
# ============================================================================
AIRFLOW_IMAGE_NAME=apache/airflow:2.11.0-python3.10
AIRFLOW_UID=50000
AIRFLOW_PROJ_DIR=.

# Airflow Web UI Credentials
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# Airflow Core Settings
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
AIRFLOW__CORE__UNIT_TEST_MODE=false

# Additional Python Requirements
# Add any additional Python packages you need here
# Example: _PIP_ADDITIONAL_REQUIREMENTS=pandas==2.0.0 requests==2.31.0
_PIP_ADDITIONAL_REQUIREMENTS=

# ============================================================================
# Database Configuration
# ============================================================================
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# ============================================================================
# Redis Configuration
# ============================================================================
REDIS_PASSWORD=

# ============================================================================
# Market Data DAG Configuration
# ============================================================================

# Default tickers for market data DAG (JSON array or CSV)
MARKET_DATA_DEFAULT_TICKERS=["AAPL","MSFT"]

# Storage Configuration
MARKET_DATA_STORAGE_DIR=/opt/airflow/data

# API Configuration
YAHOO_FINANCE_API_BASE_URL=https://query2.finance.yahoo.com/v8/finance/chart

# Retry Configuration
MARKET_DATA_API_TIMEOUT=30
MARKET_DATA_MAX_RETRIES=3
MARKET_DATA_RETRY_DELAY=5

# Sensor Configuration
MARKET_DATA_SENSOR_POKE_INTERVAL=30
MARKET_DATA_SENSOR_TIMEOUT=600
MARKET_DATA_SENSOR_EXPONENTIAL_BACKOFF=true

# Backfill Configuration
MARKET_DATA_BACKFILL_DAYS=120

# ============================================================================
# Logging Configuration
# ============================================================================

# Environment (affects log level and warehouse selection)
# Options: development | staging | production
# - development: DEBUG logs + PostgreSQL warehouse
# - staging: INFO logs + Redshift staging warehouse
# - production: WARNING logs + Redshift production warehouse
ENVIRONMENT=development

# Explicit Log Level (overrides environment-based level)
# Options: DEBUG|INFO|WARNING|ERROR|CRITICAL
# AIRFLOW__LOGGING__LEVEL=INFO

# JSON Log Format (useful for log aggregation tools)
# Options: true|false
AIRFLOW__LOGGING__JSON_FORMAT=false

# ============================================================================
# Monitoring and Error Tracking (Optional)
# ============================================================================
# Note: Sentry and Datadog are not integrated by default.
# To add these integrations, see: docs/user-guide/logging.md

# Sentry Configuration (requires extending logger module)
# SENTRY_DSN=https://your-key@sentry.io/project-id
# SENTRY_TRACES_SAMPLE_RATE=0.1
# SENTRY_SEND_PII=false

# Datadog Configuration (requires extending logger module)
# DD_API_KEY=your-datadog-api-key
# DD_APP_KEY=your-datadog-app-key
# DD_SITE=datadoghq.com
# DD_SERVICE=airflow-market-data

# ============================================================================
# DATA WAREHOUSE CONFIGURATION
# ============================================================================

# Note: ENVIRONMENT variable (defined above in Logging section) controls which warehouse to use
# - development → PostgreSQL (DEV_WAREHOUSE_*)
# - staging → Redshift (STAGING_WAREHOUSE_*)
# - production → Redshift (PROD_WAREHOUSE_*)

# --- DEVELOPMENT WAREHOUSE (PostgreSQL) ---
# Dedicated PostgreSQL service for development warehouse (separate from Airflow metadata)

DEV_WAREHOUSE_TYPE=postgresql
DEV_WAREHOUSE_HOST=warehouse-postgres
DEV_WAREHOUSE_PORT=5432
DEV_WAREHOUSE_DATABASE=market_data_warehouse
DEV_WAREHOUSE_SCHEMA=public
DEV_WAREHOUSE_USER=warehouse_user
DEV_WAREHOUSE_PASSWORD=CHANGE_ME_dev_warehouse_password

# --- STAGING WAREHOUSE (Redshift) ---

STAGING_WAREHOUSE_TYPE=redshift
STAGING_WAREHOUSE_HOST=your-redshift-cluster.region.redshift.amazonaws.com
STAGING_WAREHOUSE_PORT=5439
STAGING_WAREHOUSE_DATABASE=market_data_staging
STAGING_WAREHOUSE_SCHEMA=public
STAGING_WAREHOUSE_USER=CHANGE_ME_staging_username
STAGING_WAREHOUSE_PASSWORD=CHANGE_ME_staging_password
STAGING_WAREHOUSE_REGION=us-east-1

# --- PRODUCTION WAREHOUSE (Redshift) ---

PROD_WAREHOUSE_TYPE=redshift
PROD_WAREHOUSE_HOST=your-redshift-prod-cluster.region.redshift.amazonaws.com
PROD_WAREHOUSE_PORT=5439
PROD_WAREHOUSE_DATABASE=market_data_prod
PROD_WAREHOUSE_SCHEMA=public
PROD_WAREHOUSE_USER=CHANGE_ME_prod_username
PROD_WAREHOUSE_PASSWORD=CHANGE_ME_prod_password
PROD_WAREHOUSE_REGION=us-east-1

# --- WAREHOUSE CONFIGURATION ---

# Table names (consistent across environments)
WAREHOUSE_TABLE_MARKET_DATA=fact_market_data
WAREHOUSE_TABLE_TICKERS=dim_tickers
WAREHOUSE_TABLE_DATES=dim_dates

# Load strategy
WAREHOUSE_LOAD_STRATEGY=upsert  # Options: append, upsert, truncate_insert
WAREHOUSE_BATCH_SIZE=1000
WAREHOUSE_ENABLE_PARTITIONS=true

# Connection pool
WAREHOUSE_POOL_SIZE=5
WAREHOUSE_MAX_OVERFLOW=10
WAREHOUSE_POOL_TIMEOUT=30

